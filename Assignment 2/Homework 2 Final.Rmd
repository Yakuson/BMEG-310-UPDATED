---
title: "Homework 2 Final"
author: "Charlene Harasym, Jackson Chen, Kira Li"
date: "14/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
ovarian.dataset <- read.delim(choose.files(), sep=",", header = FALSE)
features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", paste("protein", seq(1, 25), sep=""))
names(ovarian.dataset) <- c("cell_id", "diagnosis", features) # paste0(features,"_mean"), paste0(features,"_se"), paste0(features,"_worst"))

head(ovarian.dataset)
```
Q1.1
```{r}
ovarian.dataset.pca <- prcomp(ovarian.dataset[,c(3:32)], center = TRUE,scale. = TRUE)
summary(ovarian.dataset.pca)
```
42.77 % of the variation in the data is associated with PC1

Q1.2
You would need 9 PC's in order to represent 90% of the variance in the data. 


Q1.3
```{r}
ovarian.dataset.celltype<-ovarian.dataset[,2]
library(ggbiplot)
ggbiplot(ovarian.dataset.pca,ellipse=TRUE, groups=ovarian.dataset.celltype)

```
Q1.4
```{r}

ggplot(ovarian.dataset, aes(x=area,y=concavity))+
     geom_point(aes(color=diagnosis))
```
Q1.5
The first plot has the colours for tumor type separated mostly, while in the second plot the tumor types are mixed together. The first graph gives you better separation between classes because PCA was performed in order to group the variations in the data. This is taking into account all of the different data parameters. The second graph is only graphing area vs concavity, which is only 2 of the data parameters.

Q1.6 
```{r}
boxplot(ovarian.dataset.pca$x)
```

Q2.1
```{r}
scaled <- scale(ovarian.dataset[,3:32])
km <- kmeans(scaled, centers=2)
km_labels <- ifelse(km$cluster == 1, "M", "B")
true_labels <- ovarian.dataset$diagnosis
cm <- as.matrix(table(km_labels, true_labels))
accuracy <- sum(diag(cm))/sum(cm)
precision <- cm[2,2]/(cm[2,2]+cm[2,1])
recall <- cm[2,2]/(cm[2,2]+cm[1,2])
print(paste("Accuracy= ", accuracy))
print(paste("Precision= ", precision))
print(paste("Recall= ", recall))
```

Q2.2
```{r}
accuracies <- vector()
for (i in 1:10){
  km <- kmeans(scaled, 2)
  km_labels <- ifelse(km$cluster == 1, "M", "B")
  true_labels <- ovarian.dataset$diagnosis
  cm <- as.matrix(table(km_labels, true_labels))
  accuracy <- sum(diag(cm))/sum(cm)
  accuracies <- c(accuracies, accuracy)
}
print("Accuracies across 10 runs: ")
print(accuracies)
print(paste("Mean accuracy across 10 runs= ", mean(accuracies)))
```
The results are different each run because the labeling of "M" and "B" doesn't always match the diagnosis labels, when they match the accuracy is high and when they're reversed the accuracy is low. To consistently get the correct accuracy, take the larger of the accuracy score given and 1-accuracy:
```{r}
accuracy <- max(accuracy, 1-accuracy)
```
Then, the mean accuracy across 10 runs is the same as in Q2.1:
```{r}
accuracies <- vector()
for (i in 1:10){
  km <- kmeans(scaled, 2)
  km_labels <- ifelse(km$cluster == 1, "M", "B")
  true_labels <- ovarian.dataset$diagnosis
  cm <- as.matrix(table(km_labels, true_labels))
  accuracy <- sum(diag(cm))/sum(cm)
  accuracy <- max(accuracy, 1-accuracy)
  accuracies <- c(accuracies, accuracy)
}
print("Accuracies across 10 runs: ")
print(accuracies)
print(paste("Mean accuracy across 10 runs= ", mean(accuracies)))
```

Q2.3
```{r}
top5pc <- prcomp(ovarian.dataset[features], center = TRUE,scale. = TRUE, rank. =5)
accuracies <- vector()
for (i in 1:10){
  km <- kmeans(top5pc$x, centers=2)
  km_labels <- ifelse(km$cluster == 1, "M", "B")
  true_labels <- ovarian.dataset$diagnosis
  cm <- as.matrix(table(km_labels, true_labels))
  accuracy <- sum(diag(cm))/sum(cm)
  accuracy <- max(accuracy, 1-accuracy)
  accuracies <- c(accuracies, accuracy)
}
print("Accuracies across 10 runs: ")
print(accuracies)
print(paste("Mean accuracy across 10 runs= ", mean(accuracies)))
```

Q2.4
The results are slightly worse with PCA compared to scaled data because the top 5 PCs capture most of the variation (~80%) but not all, whereas the scaled data should capture all of the variation. 

Q3.1
```{r}
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]

ovarianStuff.glm <- glm(as.factor(diagnosis)~. -cell_id, data = ovarian.dataset.train, family = binomial)
ovarianStuff.Predict1 <- predict(ovarianStuff.glm, ovarian.dataset.test, type = "response")
ovarianStuff.Predict2 <- ifelse(ovarianStuff.Predict1 > 0.5, "M", "B")

#ovarianStuff.Predict2 <- ifelse(ovarianStuff.Predict2 == "M", 1, 0)


confusion_Mat <- table(ovarianStuff.Predict2, ovarian.dataset.test$diagnosis)

library(MLmetrics)

test.accuracy <- Accuracy(ovarianStuff.Predict2, ovarian.dataset.test$diagnosis)
#Used functions from MLmetrics library

test.precision <- Precision(ifelse(ovarianStuff.Predict2=="M",1,0), ifelse(ovarian.dataset.test$diagnosis=="M",1,0))
test.recall <- Recall(ifelse(ovarianStuff.Predict2=="M",1,0),ifelse(ovarian.dataset.test$diagnosis=="M",1,0))

ovarianStuff.train1 <- predict(ovarianStuff.glm, ovarian.dataset.train, type = "response")
ovarianStuff.train2 <- ifelse(ovarianStuff.train1 > 0.5, "M", "B")

train.accuracy = Accuracy(ovarianStuff.train2, ovarian.dataset.train$diagnosis)


print("Test set")
print(paste("Accuracy =", test.accuracy))
print(paste("Precision =", test.precision))
print(paste("Recall =", test.recall))
print(paste("Training Accuracy = ", train.accuracy))


```
*Note, external library MLmetrics was used

The training set gives better results because the model was originally optimized for the training data.

Q3.2
```{r}
ovarian.pca.top5 <- as.data.frame(ovarian.dataset.pca$x[,1:5])
ovarian.pca.top5$diagnosis <- ovarian.dataset$diagnosis

#Train and tests with top 5 pcas
ovarian.pca.train <- ovarian.pca.top5[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.pca.test <- ovarian.pca.top5[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
#

ovarian.pca.glm <- glm(as.factor(diagnosis)~., data = ovarian.pca.train, family = binomial)

ovarian.pca.Predict1 <- predict(ovarian.pca.glm, ovarian.pca.test, type = "response")
ovarian.pca.Predict2 <- ifelse(ovarian.pca.Predict1 > 0.5, "M", "B")

test.top5pca.accuracy <-
  Accuracy(ovarian.pca.Predict2, ovarian.pca.test$diagnosis)
#Used functions from MLmetrics library
test.top5pca.precision <- Precision(ifelse(ovarianStuff.Predict2=="M",1,0), ifelse(ovarian.dataset.test$diagnosis=="M",1,0))
test.top5pca.recall <- Recall(ifelse(ovarianStuff.Predict2=="M",1,0),ifelse(ovarian.dataset.test$diagnosis=="M",1,0))


print("Test set")
print(paste("Top 5 Accuracy =", test.top5pca.accuracy))
print(paste("Top 5 Precision =", test.top5pca.precision))
print(paste("top 5 Recall =", test.top5pca.recall))

```
Q3.3 
The results from the top5 PCA logistic regression are slightly better than the previous results. This is probably due to the reduction of non-accurate information contained in the test set which decreased the chance of the algorithm overfitting. 

Q3.4
The classification method (logarithmic regression), gave us better results compared to the clustering method (K-means). As our logarithmic regression method is a supervised ML method, we expressly supplied the algorithm with training set data that was already extremely close to expected values. In contrast, the K-means method was at a disadvantage since it needed to determine trends and correlations by itself. 

Q3.5
```{r}
library(ROCR)
pred.prob <- predict(ovarianStuff.glm, ovarian.dataset, type="response")
predict <- prediction(pred.prob, ovarian.dataset$diagnosis, label.ordering=c("B","M"))
perform <- performance(predict,"tpr","fpr")
plot(perform,colorize=TRUE)
```
Given the generated ROC curve and the sharp trend towards AUC near to the 1, our model has a good measure of separability. Additionally, the ROC curve can inform us on how the model's accuracy performs over time.


*Note: Installed external library randomForest for 3.6

```{r}
library(randomForest)
ovarian.Q6 <- randomForest(as.factor(diagnosis)~. -cell_id, data=ovarian.dataset.train, importance = TRUE)
ovarian.Q6.Predict1 <- predict(ovarian.Q6, ovarian.dataset.test, type = "response")

#MLmetrics library again:
Q6.test.accuracy <- Accuracy(ovarian.Q6.Predict1, ovarian.dataset.test$diagnosis)
Q6.test.precision <- Precision(ovarian.Q6.Predict1, ovarian.dataset.test$diagnosis)
Q6.test.recall <- Recall(ovarian.Q6.Predict1, ovarian.dataset.test$diagnosis)

ovarian.Q6.Predict.Train <- predict(ovarian.Q6, ovarian.dataset.train, type = "response")

Q6.train.accuracy <- Accuracy(ovarian.Q6.Predict.Train, ovarian.dataset.train$diagnosis)

#Top 5 PCA portion:
ovarian.Q6.top5 <- randomForest(as.factor(diagnosis)~., data=ovarian.pca.train, importance = TRUE)

ovarian.Q6.top5.Predict1 <- predict(ovarian.pca.glm, ovarian.pca.test, type = "response")
ovarian.Q6.top5.Predict2 <- ifelse(ovarian.pca.Predict1 > 0.5, "M", "B")

Q6.pca.test.accuracy <- Accuracy(ovarian.Q6.top5.Predict2, ovarian.pca.test$diagnosis)
Q6.pca.test.precision <- Precision(ovarian.Q6.top5.Predict2, ovarian.pca.test$diagnosis)
Q6.pca.test.recall <- Recall(ovarian.Q6.top5.Predict2, ovarian.pca.test$diagnosis)


print("Q3.6 Answers: ")
print(paste("Test Accuracy: ", Q6.test.accuracy))
print(paste("Test Precision: ", Q6.test.precision))
print(paste("Test Recall: ", Q6.test.recall))
print(paste("Training Accuracy: ", Q6.train.accuracy))
print(paste("Top 5 PCA Test Accuracy: ", Q6.pca.test.accuracy))
print(paste("Top 5 PCA Test Precision: ", Q6.pca.test.precision))
print(paste("Top 5 PCA Test Recall: ", Q6.pca.test.recall))



```
Q3.6 (Repeat of 3.3)
This time, our results became less accurate when we only used the top 5 PCA values. This may be because the random forest attempted to overfit more when given fewer parameters. 
